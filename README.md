# Policy-Improvement-Methods-Analysis

A comprehensive analytical study comparing the performance of various policy improvement methods across reinforcement learning environments. This project implements, evaluates, and visualizes different exploration strategies to understand their convergence properties and performance characteristics.

##  Research Focus

This project systematically analyzes policy improvement algorithms in RL, with emphasis on:

- **Exploration-Exploitation Tradeoff**: How different methods balance discovering new actions vs exploiting known rewards
- **Convergence Properties**: Analysis of how quickly policies learn optimal behavior
- **Hyperparameter Sensitivity**: Study of how parameters like ε affect performance
- **Environment Adaptation**: How methods perform across different problem types (N-chain, MDPs)
- **Theoretical vs Practical Performance**: Comparing analytical guarantees with empirical results

##  Key Analyses
- **Regret Analysis**: Cumulative performance loss vs optimal
- **Convergence Speed**: Time to discover optimal policies  
- **Parameter Studies**: ε sensitivity, exploration constants
- **Statistical Validation**: Significance testing across multiple runs
- **Visual Comparisons**: Learning curves and policy evolution
